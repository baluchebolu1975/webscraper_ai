"""
Simple demo to show actual web scraping with real results.
"""
import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.scraper import WebScraper
from src.utils import save_to_json, get_timestamp
import json


def demo_real_scraping():
    """Demonstrate actual web scraping with a working URL."""
    print("â•”" + "â•" * 58 + "â•—")
    print("â•‘" + " " * 15 + "WEB SCRAPING DEMONSTRATION" + " " * 16 + "â•‘")
    print("â•š" + "â•" * 58 + "â•\n")
    
    # Initialize scraper with custom config
    scraper = WebScraper(timeout=10, max_retries=2)
    
    # Use a simple, reliable test URL
    url = "http://info.cern.ch/"  # First website ever created, still online!
    
    print(f"ğŸŒ Target URL: {url}")
    print(f"âš™ï¸  Configuration:")
    print(f"   â€¢ Timeout: {scraper.timeout}s")
    print(f"   â€¢ Max Retries: {scraper.max_retries}")
    print(f"\n{'â”€' * 60}")
    print("ğŸ“¡ Starting scrape operation...\n")
    
    try:
        # Perform the scrape
        data = scraper.scrape(url)
        
        if data and 'error' not in data:
            print("âœ… SCRAPING SUCCESSFUL!\n")
            print("ğŸ“Š Results Summary:")
            print(f"   â€¢ Page Title: {data.get('title', 'N/A')}")
            print(f"   â€¢ Text Length: {len(data.get('text', ''))} characters")
            print(f"   â€¢ Links Found: {len(data.get('links', []))} links")
            print(f"   â€¢ Images Found: {len(data.get('images', []))} images")
            
            # Show first few links
            links = data.get('links', [])
            if links:
                print(f"\nğŸ”— Sample Links (first 5):")
                for i, link in enumerate(links[:5], 1):
                    print(f"   {i}. {link}")
            
            # Show extracted text preview
            text = data.get('text', '')
            if text:
                print(f"\nğŸ“„ Text Preview (first 200 chars):")
                print(f"   \"{text[:200].strip()}...\"")
            
            # Save to file
            filename = f"demo_scrape_{get_timestamp()}.json"
            filepath = save_to_json(data, filename)
            print(f"\nğŸ’¾ Full results saved to:")
            print(f"   {filepath}")
            
            # Display JSON structure
            print(f"\nğŸ“‹ JSON Structure:")
            print(json.dumps({
                'url': data['url'],
                'title': data.get('title', 'N/A'),
                'text_length': len(data.get('text', '')),
                'links_count': len(data.get('links', [])),
                'images_count': len(data.get('images', []))
            }, indent=2))
            
        else:
            error = data.get('error', 'Unknown error')
            print(f"âŒ SCRAPING FAILED: {error}")
    
    except Exception as e:
        print(f"âŒ ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
    
    finally:
        scraper.close()
        print(f"\n{'â”€' * 60}")
        print("âœ“ Scraper session closed")


if __name__ == "__main__":
    demo_real_scraping()
    print("\nğŸ‰ Demo completed!\n")
